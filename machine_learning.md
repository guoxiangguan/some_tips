# machine_learning notebook

## sklearn

### 常用包
```python
from sklearn import datasets
from sklearn import svm
from joblib import dump, load
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
```

### 加载数据集
```python
iris = datasets.load_iris()
digits = datasets.load_digits()
```

### 特征工程以及数据集的选取
```python
X, y = iris.data, iris.target
# 根据多方面的评估选择最好的 k 个特征
sel = SelectKBest(chi2, k=3) # 针对分类型任务
X_sel = sel.fit_transform(X, y)
# 随机分离训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)
```
* 非数字列处理
    `data_train.loc[data_train["Sex"] == "male","Sex"] = 0` # 将满足某个条件的元素重新赋值

#### 训练集和测试集的选取
* 留出法
    `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)`
* k 折交叉验证
```python
from sklearn import model_selection
#逻辑回归
from sklearn.linear_model import LogisticRegression   
#初始化逻辑回归算法
LogRegAlg=LogisticRegression(random_state=1)
re = LogRegAlg.fit(data_train[predictors],data_train["Survived"])
#使用sklearn库里面的交叉验证函数获取预测准确率分数
scores = model_selection.cross_val_score(LogRegAlg,data_train[predictors],data_train["Survived"],cv=3)
#使用交叉验证分数的平均值作为最终的准确率
print("准确率为: ",scores.mean())
```

#### 数据处理
* 如果某列存在大量的空值, 并且对结果的判断关系不大的话, 可以删去
* 如果某一列对结果的判断挺重要的, 可以选择平均值填充或者出现频率最多的值填充或者最大值进行填充
    `data_train["Age"] = data_train['Age'].fillna(data_train['Age'].median())`
    `data_test["Fare"] = data_test["Fare"].fillna(data_test["Fare"].max())`
* 如果某列即使不存在大量空值, 例如 id, 但与结果完全没有关系, 可以删去
* 虚拟化变量
    `X = pd.get_dummies(X) # 将所有的分类型特征转换为数字, 虚拟变量: dummy variables`

### learning and predicting
```python
'''
clf = svm.SVC(gamma=0.001, C=100.)
clf.fit(digits.data[:-1], digits.target[:-1])
pre = clf.predict(digits.data[-1:])
'''
model = LogisticRegression()
model.fit(X_train, y_train)
pre = model.predict(X_test)
metric = accuracy_score(y_test, pre)
print('metric: ',metric)
```

#### 随机森林
```python
from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier
predictors=["Pclass","Sex","Age","SibSp","Parch","Fare","Embarked"]
#10棵决策树，停止的条件：样本个数为2，叶子节点个数为1
alg=RandomForestClassifier(random_state=1,n_estimators=10,min_samples_split=2,min_samples_leaf=1) 
#Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)
#kf=cross_validation.KFold(data_train.shape[0],n_folds=3,random_state=1)
kf=model_selection.KFold(n_splits=3,shuffle=False, random_state=1)
scores=model_selection.cross_val_score(alg,data_train[predictors],data_train["Survived"],cv=kf)
print(scores)
#Take the mean of the scores (because we have one for each fold)
print(scores.mean())
```

#### Lasso
```python
from sklearn import linear_model
reg = linear_model.Lasso(alpha=0.1)
reg.fit([[0, 0], [1, 1]], [0, 1])  
reg.predict([[1, 1]])
array([0.8])
```

### model persistence
```python
clf = svm.SVC(gamma='scale')
X, y = iris.data, iris.target
clf.fit(X, y)
s = pickle.dumps(clf)
clf2 = pickle.loads(s)
print(clf2.predict(X[0:1]))
print(y[0])
dump(clf, 'clf.joblib')
clf = load('clf.joblib')
```

## 常见算法

### 集成学习
* 根据个体学习器的生成方式, 目前的集成学习方法大致可分为两大类, 即个体学习器间存在强依赖关系, 必须串行生成的序列化方法, 以及个体学习器间不存在强依赖关系, 可同时生成的并行化的方法; 前者的代表是 Boosting, 后者的代表是 Bagging 和 "随机森林" (Random Forest)

#### Boosting
* Boosting 是一族可将弱学习器提升为强学习器的算法.
* 工作机制: 先从初始训练集训练出一个基学习器, 再根据基学习器的表现对训练样本分布进行调整, 使得先前基学习器做错的训练样本再后续受到更多关注, 然后基于调整后的样本分布来训练下一个基学习器; 如此反复进行, 直至基学习器数目达到事先指定的值 T, 最终将这 T 个基学习器进行加权结合.
* Boosting 族算法最著名的代表是 AdaBoost.
* Boosting 算法要求基学习器能对特定的数据分布进行学习, 这可通过 "重新赋权法" (re-weighting) 实施, 即在训练过程的每一轮中, 根据样本分布为每个训练样本重新赋予一个权重. 对无法接受带权样本的基学习算法, 则可以通过 "重采样法" (re-sampling) 来处理, 即在每一轮学习中, 根据样本分布对训练集重新进行采样, 再用重采样而得到的样本集对基学习器进行训练.
* 从偏差-方差分解的角度看, Boosting 主要关注降低偏差, 因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成.

### Bootstrap aggregating (also called Bagging)
* 简介
    * 用于提高机器学期算法的稳定性和精度, 这些机器学习算法通常是用在统计分类和回归问题.
    * 可以降低偏差并且有助于避免过拟合
    * 经常用在决策树上, 理论上适用任何类型的算法
    * 采用 bootstrap sample, 即有放回的采样方式
    * 对于回归问题用取平均值的方式, 对分类问题用投票的方式
    * 它会略微地降低一些稳定算法的表现, 比如最近邻 (K-nearest neighbors)

### 随机森林 (random decision forests)
* 简介
    * 结合随机节点优化和 bagging, 利用类 CART 过程构建不相关数的森林的方法
    * 使用 out-of-bag 误差来代替泛化误差, 即没有采样到的样本作为测试集估计泛化误差
    * 通过排列度量变量的重要性
    * 使用一种改进的学习算法, 在学习过程中的每次候选分裂中选择特征的随机子集
    * 随机森林天然可用来对回归或分类问题中的变量的重要性进行排序
### AdaBoost
- 加法模型, 损失函数为指数函数, 学习算法为前向分步算法时的二分类学习方法. 加法模型即基分类器的线性组合.

### 前向分布算法
- 给定训练数据和损失函数的条件下, 学习加法模型成为经验风险极小化, 即最小化每一步生成的基函数的损失函数之和, 但是这通常是一个很复杂的问题, 因此提出前向分步算法的思想.
- 由于学习的是加法模型, 如果能从前向后每一步只学习一个基函数及其系数, 逐步逼近优化目标函数式, 那么就可以简化优化的复杂度.也就是说每次学习一个基函数, 只针对这个基分类器进行优化, 使其损失函数最小.


### 随机森林(Random Forest)
- Bagging 的一种扩展变体, 在以决策树为基学习器构建 Bagging 集成的基础上, 进一步在决策树的训练过程中引入了随机属性选择.
- 传统的决策树在选择划分属性时是在当前节点的属性集合中选择一个最优属性, 而在 RF 中, 对基学习器的每个节点, 先从该结点的属性集合中随机选择一个包含 k 个属性的子集, 然后再从这个子集中选择一个最优属性用于划分, 推荐值 k=log2d.
- 随机森林中基学习器的多样性不仅来自样本扰动, 还来自属性扰动, 这就使得最终集成的泛化性能可通过个体学习器之间差异值的增加而进一步提升.



### 基于二次准则函数的 H-K 算法
* 基于二次准则函数的 H-K 算法较之于感知器算法的优点
    1. 可以判别问题是否线性可分
    2. 其解得适应性更好

### 感知器算法

### K-means 聚类算法
* K-means 聚类算法的特点
    1. 对大数据集有较高的效率并且具有可伸缩性
    2. 是一种无监督的学习方法
    3. k 值无法自动获取, 初始聚类中心随机选择

### PCA (主成分分析)
* 主要去做的事
    1. 可以找到相互独立的特征
        * 若先用 PCA, 再用朴素贝叶斯, 则效果比单独用朴素贝叶斯效果好很多.
    2. 降维
* 推导过程
    1. 先中心化数据
        - `A - np.mean(A)` (方便计算方差以求出数据的分布情况)
    2. 计算投影后的数据方差最大值

## LDA (Linear Discriminant Analysis, 线性判别分析)
- 线性判别分析是一种经典的线性学习在二分类问题上因为最早由 Fisher 提出, 亦称为 Fisher 判别分析.
- 严格来说 LDA 与 Fisher 判别分析稍有不同, 前者假设了各类样本的协方差矩阵相同且满秩.
- LDA 的思想非常朴素: 给定训练样例集, 设法将样例投影到一条直线上, 使得同类别样例的投影点尽可能接近, 异类样例的投影尽可能远离; 对新样本进行分类时, 将其投影到同样的这条直线上, 再根据投影点的位置来确定新样本的类别.
- 欲使同类样例的投影点尽可能接近, 可以让同类样例投影点的协方差尽可能小; 而欲使异类样例的投影点尽可能远离, 可以让类中心之间的距离尽可能大.
- LDA 欲最大化的目标是类内散度矩阵和类间散度矩阵的广义瑞利商.
- LDA 可从贝叶斯决策理论的角度来阐释, 并可证明, 当两类数据同先验, 满足高斯分布且协方差相等时, LDA 可达到最优分类.
- 在推广到多分类任务中, 可通过投影来减小样本点的维数, 且投影过程中使用了类别信息, 因此 LDA 也常被视为一种经典的监督降维技术.



### EM 算法
#### 简介
* EM 算法称为期望最大值算法, 是无监督的算法, 通过逐步提高极大似然的下限, 以此求出极大似然函数对参数的估计, 为无监督算法.
* 是一种族算法(cluster algorithm)

### 路径搜索
* A* 算法, 俗称 A 星算法, 这是一种在图形平面上, 有多个节点的路径, 求出最低通过成本的算法.
    * 博客地址: [A星算法详解](https://blog.csdn.net/hitwhylz/article/details/23089415)
* Dijkstra
    * 使用广度优先搜索解决赋权有向图的单源最短路径问题

## 机器学习相关软件包

### SPASS
* SPASS 是统计产品与服务解决方案 (Statistical Product and Service Solutions) 的简称, 为 IBM 公司的一系列用于统计学习分析, 数据挖掘, 预测分析和决策支持任务的软件产品及相关服务的总称.
* SPASS 界面的主窗口为 *数据编辑窗口*

## 机器学习常识

### 可以用来降维的算法
1. Latent Dirichlet Allocation: 把文档投影到 "topic" 空间, 可以理解为降维.
2. word2vec: 在给定的语料库上训练一个模型, 输出出现在语料库中单词的向量 (word embedding) 表示, NLP 中传统的词表示方法是把每个单词表示成 dim (词汇量大小) 维的稀疏向量, 这个稀疏向量只有一个维度 (该单词的 index) 上是1, 其余全是0, 单词之间孤立, word embedding 则是把单词的表示降维到 n 维的稠密向量, n << dim.
3. PCA
4. 自编码: 隐藏层的神经元数目少于输入就可以看做降维和压缩.

### 偏差和方差
* 偏差: 描述的是预测值 (估计值) 的期望与真实值之间的差距. 偏差越大, 越偏离真实数据
* 方差: 描述的是预测值的变化范围, 离散程度, 也就是离其期望值的距离. 方差越大, 数据的分布越分散.

### 训练集
* 监督学习中的训练集用于估算模型

### 二项分布
- 均值: np; 方差: np(1-p); 标准差: 1/n * (np(1-p)) ^ 1/2

## 模型
* 生成式模型:
    * 判别式分析
    * 朴素贝叶斯
    - K 近邻
    - 混合高斯模型
    - 隐马尔科夫模型
    - 贝叶斯网络
    - Sigmoid Belief Networks
    - 马尔科夫随机场 (Markow Random Fields)
    - 深度信念网络(DBN)
- 判别式模型
    - 线性回归(Linear Regression)
    - Logistic Regression
    - 神经网络
    - 支持向量机
    - 高斯过程
    - 条件随机场(CRF)
    - CART(Classification and Regression Tree)
    - 最大熵

### 生成式模型和判别式模型 (Generative Model and Discrimitive Model)
* 区别 (对于输入 x, 类别标签y):
    1. 生成式模型估计它们的联合概率 P(x, y) 
    2. 判别式模型估计决策函数 F(x) 或条件概率分布 P(y|x)
    3. 生成式模型可以根据贝叶斯公式得到判别式模型, 但反过来不行
* 常见的生成式模型和判别式模型
    * 生成式模型
        1. 判别式分析
        2. 朴素贝叶斯Native Bayes
        3. 混合高斯型Gaussians
        4. K近邻KNN
        5. 隐马尔科夫模型HMM
        6. 贝叶斯网络
        7. sigmoid belief networks
        8. 马尔科夫随机场Markov random fields
        9. 深度信念网络DBN
        10. 隐含狄利克雷分布简称LDA(Latent Dirichlet allocation)
        11. 多专家模型(the mixture of experts model)
    * 判别式模型
        1. 线性回归linear regression
        2. 逻辑回归logic regression
        3. 神经网络NN
        4. 支持向量机SVM
        5. 高斯过程Gaussian process
        6. 条件随机场CRF
        7. CART(Classification and regression tree)
        8. Boosting

### 决策树
#### 信息增益
1. 计算公式:

### CRF 模型和 HMM 以及 MEMM 模型之间的关系
1. CRF 没有 HMM 那样严格的独立性假设条件, 因此可以容纳任意的上下文信息. 与 ME 一样, 特征设计灵活. ---- 与 HMM 比较
2. 由于 CRF 计算全局最优输出节点的条件概率, 它还克服了最大熵马尔科夫模型标记偏置的缺点. ---- 与 MEMM 比较
3. CRF 是在给定需要标记的观察序列的条件下, 计算整个标记序列的联合概率分布, 而不是在给定当前状态条件下, 定义下一个状态的状态分布. ---- 与 ME 比较
4. 缺点: 训练代价大, 复杂度高.

### 隐马尔科夫模型 (Hidden Markov Model; 缩写: HMM)
* 简介:
    * 是统计模型, 用于描述一个含有隐含未知参数的马尔科夫过程.
    * 其难点在于从可观察的参数中确定该过程的隐含参数.

### 隐马尔科夫模型的三个基本问题以及相应的算法
1. 前向, 后向算法解决的是一个评估问题, 即给定一个模型, 求某特定观测序列的概率, 用于评估该序列最匹配的模型.
2. Baum-Welch 算法解决的是一个模型训练问题, 即参数估计, 是一种无监督的训练方法, 主要通过 EM 迭代实现.
3. 维特比算法 (Viterbi) 解决的是给定一个模型和某个特定的输出序列, 求最可能产生这个输出的状态序列. 如通过海藻变化 (输出序列) 来观测天气 (状态序列), 是预测问题, 通信中的解码问题.

### 时间序列模型
1. AR 模型: 自回归模型, 是一种线性模型.
2. MA 模型: 移动平均模型, 其中使用趋势移动平均建立直接趋势的预测模型.
3. ARMA 模型: 自回归滑动平均模型, 拟合较高阶模型.
4. CARCH 模型: 广义回归模型, 对误差的方差建模, 适用于波动性的分析和预测.

### 应用
* 条件随机场和隐马尔科夫模型是典型的用来做序列标注问题的模型, 是构建语音识别声学模型的传统方法, RNN 因含有递归层而具有序列结构的特点, 现在被广泛用于语音识别等序列标注问题中.

### 文本分类中常用的特征选择方法
* 常采用特征选择方法。常见的六种特征选择方法：
    1. DF(Document Frequency) 文档频率
    DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性
    2. MI(Mutual Information) 互信息法
    互信息法用于衡量特征词与文档类别直接的信息量。
    如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。
    相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。
    3. (Information Gain) 信息增益法
    通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。
    4. CHI(Chi-square) 卡方检验法
    利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的
    如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。
    5. WLLR(Weighted Log Likelihood Ration)加权对数似然
    6. WFO（Weighted Frequency and Odds）加权频率和可能性

### 概率
* 全概率公式: P(X =1) = P(X = 1|Y = 1) * P(Y = 1) + P(X = 1|Y = 0) * P(Y = 0)

### 统计模式分类问题
* 当先验概率未知时，可以使用最小最大损失准则和N-P判决.
    最小损失准则中需要用到先验概率
    最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。
    在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 
    1. p(y)已知，直接使用贝叶斯公式求后验概率即可； 
    2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。

### 有效解决过拟合的方法
1. 增加样本数量
2. 采用正则化的方法
* 增加特征数量和迭代更多的轮次不能有效解决过拟合

### 数据清理中, 处理缺失值的方法
1. 估算
2. 整例删除
3. 变量删除
4. 成对删除

### tanh
* tanh 的值域在 (-1,1)

### logistic regression
* 逻辑回归主要用于解决分类问题

### 从使用的主要技术上看，可以把分类方法归结为哪几种类型 
* 从使用技术上来分，可以分为四种类型：基于距离的分类方法、决策树分类方法、贝叶斯分类方法和规则归纳方法。基于距离的分类方法主要有最邻近方法；决策树方法有ID3、C4.5、VFDT等；贝叶斯方法包括朴素贝叶斯方法和EM算法；规则归纳方法包括AQ算法、CN2算法和FOIL算法。

### 一些降维方法
1. LASSO: 通过参数缩减达到降维的目的
2. PCA
3. 聚类分析
4. 小波分析: 有一些变换的操作降低干扰可以看做降维
5. 线性判别法: 通过找到一个空间使得类内距离最小, 类间距离最大
6. 拉普拉斯特征映射

### one-hot [one-hot encoding 详解](https://www.jianshu.com/p/cb344e1c860a)
- 解决的问题: 很多机器学习任务中, 特征并不总是连续值, 有可能是分类值. 将分类值用数字表示效率会高很多, 但是还是不能直接用在分类器中, 因为分类器往往默认数据是连续的, 并且是有序的.
- 热独编码, 又称一位有效编码, 其方法是使用 N 位状态寄存器来对 N 个状态进行编码, 每个状态都有它独立的寄存器位, 并且在任意时候, 其中只有一位有效.

### 特征选择
- 从给定的特征集合中选择出相关特征子集的过程, 称为 "特征选择".
- 特征选择的原因:
    1. 减轻由于属性过多而导致的维数灾难问题.
    2. 去除不相关特征, 从而降低学习任务的难度.
- 注意点:
    - 特征选择的过程中确保不丢失重要特征, 否则后续的学习过程会因为重要信息的缺失而无法获得好的性能.
- 特征选择分为子集搜索与子集评价两个部分.
    - 子集搜索的方式有三种:
        1. 逐渐增加相关特征的前向搜索策略.
        2. 从完整的特征集合开始, 每次尝试去掉一个特征, 逐渐减少特征的后向搜索策略.
        3. 结合前向后向, 每一轮逐渐增加选定相关特征(这些特征在后续不会被去除), 同时减少无关特征的双向搜索策略.
    - 子集评价: 信息增益
- 常见的特征选择方法大致分为三类: 过滤式, 包裹式, 嵌入式
    1. 过滤式方法先对数据集进行特征选择, 然后在训练学习器, 特征选择过程与后续学习器无关.
        - Relief(Relevant Features) 是一种著名的过滤式特征选择方法, 该方法设计了一个"相关统计量"来度量特征的重要性. 为二分类问题设计, 其变体 Relief-F 能处理多分类问题.
    2. 包裹式特征选择直接把最终要使用的学习器性能作为特征子集的评价准则.
        - 从最终学习器性能来看, 包裹式选择比过滤式特征选择更好, 但由于在特征选择过程中需多次训练学习器, 因此包裹式特征选择的计算开销通常比过滤式特征选择大得多.
        - LVW(Las Vegas Wrapper) 是一个典型的包裹式特征选择方法. 它在拉斯维加斯方法(Las Vegas method) 框架下使用随机策略来进行子集搜索, 并以最终分类器的误差为特征子集评价准则.
    3. 嵌入式特征选择将特征选择过程与学习器训练过程融为一体, 二者在同一个优化过程中完成, 在学习器训练过程中自动地进行了特征选择.
        - 求解 L1 范数正则化的结果是得到了仅采用一部分特征的模型, 换言之, 基于 L1 正则化的学习方法就是一种嵌入式特征选择方法, 其特征选择过程与学习器训练过程融为一体, 同时完成.
        - 求解 L1 正则化问题可以使用近端梯度下降(Proximal Gradient Descent) 简称 PGD 来求解. 具体就是将目标函数用二阶泰勒展开, 然后求出闭式解.
        
## 卡方
### 卡方分布(chi-square distribution)
- 是概率论与统计学中常用的一种概率分布.
- k 个独立的标准正态分布变量的平方和服从自由度为 k 的卡方分布.
- 卡方分布是一种特殊的伽马分布.
- 由卡方分布延申出来的皮尔森卡方检定常用于:
    1. 样本某性质的比例分布与总体理论分布的拟合优度;
    2. 同一总体的两个随机变量是否独立;
    3. 二或多个总体同一属性的同素性检定.

### 皮尔森卡方检验(Pearson's chi-squared test)
- 皮尔森卡方检验是最有名的卡方检验之一, 科学文献中, 当提及卡方检验而没有特别指明类型时, 通常即指皮尔森卡方检验.
- 皮尔森卡方检验的虚无假设(H0)是: 一个样本中已发生事件的次数分配会遵守某个特定的理论分配.
    - 在虚无假设的句子中, 事件必须互斥, 并且所有事件总几率等于1.
- 皮尔森卡方检验可用于两种情境的变项比较: 适配度检验和独立性检验
    - 适配度检验验证一组观察值的次数分配是否异于理论上的分配.
        - 测试样本的几率分配与母体有多相似.
    - 独立性检验验证从两变量抽出的配对观察值组是否相互独立.
- 检验包含三个步骤:
    1. 计算卡方检验的统计值: 把每一个观察值和理论值的差做平方后, 除以理论值, 再加总.
    2. 计算卡方统计值的自由度.
    3. 依据研究者设定的置信水平, 查出自由度为卡方统计值自由度的卡方分配临界值, 比较它与第1步骤得出的卡方统计值, 推论能否拒绝虚无假设.

## 随机抽样一致算法(RANSAC)
- 它可以从一组包含"局外点"的观测数据集中, 通过迭代方式估计数学模型的参数. 它是一种不确定的算法, 它有一定的概率得出一个合理的结果; 为了提高概率必须提高迭代次数.
- RANSAC 的基本假设:
    1. 数据由局内点组成, 例如: 数据的分布可以用一些模型参数来解释;
    2. 局外点是不能适应该模型的数据;
    3. 除此之外的数据属于噪声.
- 局外点产生的原因有: 噪声的极值; 错误的测量方法; 对数据的错误假设.
- 概述
    - RANSAC 算法的输入是一组观测数据, 一个可以解释或者适应于观测数据的参数化模型, 一些可信的参数.
    - RANSAC 通过反复选择数据中的一组随机子集来达成目标. 被选取的子集被假设为局内点, 并用下述方法进行验证:
        1. 有一个模型适应于假设的局内点, 即所有的未知参数都能从假设的局内点计算出.
        2. 用 1 中得到的模型去测试其他所有的数据, 如果某个点适用于估计的模型, 认为它也是局内点.
        3. 如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理.
        4. 然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过.
        5. 最后，通过估计局内点与模型的错误率来评估模型.
        - 这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被舍弃，要么因为比现有的模型更好而被选用.
- 应用: RANSAC 算法经常用于计算机视觉, 例如同时求解相关问题与估计立体摄像机的基础矩阵.

## seq2seq
- seq2seq 是一个 Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。
- 这个结构最重要的地方在于输入序列和输出序列的长度是可变的，可以用于翻译，聊天机器人，句法分析，文本摘要等。

## 数据挖掘算法-Apriori Algorithm(关联规则)
- [Apriori 关联规则](https://www.cnblogs.com/jingwhale/p/4618351.html)


## ROC 与 AUC
- 根据学习器的预测结果对样例排序, 按此顺序逐个把样本作为正例进行预测, 每次计算出两个最重要的值, 分别以它们为横坐标和纵坐标作图, 就得到了 ROC 曲线.
- ROC 曲线的纵轴是真正例率简称 TPR, 横轴是假正例率简称 FPR

## LRU 算法
- 全称: Least Recently Used, 即最近最久未使用的意思
- 设计原则: 如果一个数据最近一段时间内未被使用到, 那么在将来它被访问的可能性也很小. 也就是说, 当限定的空间已存满数据时, 应当把最久没有被访问到的数据淘汰.
- 实现 LRU:
    1. 用一个数组来储存数据, 给每一个数据项标记一个时间戳, 每次插入新数据项的时候, 先把数组中存在的数据项的时间戳自增, 并将新数据的时间戳置为 0 并插入到数组中. 每次访问数组中的数据项的时候, 将访问的数据项的时间戳置为 0. 当数组空间已满时, 将时间戳最大的数据项淘汰.(直接用 hash 表来存不就完事了? key 为存的值, value 为时间戳)
    2. 利用一个链表来实现, 每次新插入数据的时候将新数据插到链表的头部; 每次缓存命中(即数据被访问), 则将数据移到链表头部; 那么当链表满的时候, 就将链表尾部的数据丢弃.
    3. 利用链表和hashmap。当需要插入新的数据项的时候，如果新数据项在链表中存在（一般称为命中），则把该节点移到链表头部，如果不存在，则新建一个节点，放到链表头部，若缓存满了，则把链表最后一个节点删除即可。在访问数据的时候，如果数据项在链表中存在，则把该节点移到链表头部，否则返回-1。这样一来在链表尾部的节点就是最近最久未访问的数据项。
- AUC(Area under ROC curve): 即 ROC 曲线下方的面积.


## 哈夫曼编码
- 一种编码方式, 是可变长编码的一种, 该方法完全依据字符出现的概率来构造异字头的平均长度最短的码字, 有时称之为最佳编码.
- 基于二叉树构建编码压缩结构, 数据压缩中经典的一种算法.
1. 先按权重排序
2. 将权重最小的两个合并, 重新排列
3. 重复 1 2 的过程

## BatchNorm [batchnorm原理及代码详解](https://blog.csdn.net/qq_25737169/article/details/79048516)
- 为什么深度网络会需要 BatchNorm, 我们知道, 深度学习尤其是在 CV 上都需要对数据归一化, 因为深度神经网络主要是为了学习训练数据的分布, 并在测试集上达到很好的泛化效果, 但是, 如果每一个 Batch 输入的数据都具有不同的分布, 显然会给网络的训练带来困难. 另一方面, 数据经过一层层网络计算后, 其数据分布也在发生着变化, 此现象被称为 Internal Covariate Shift. 
-  Internal CovariateCovariateCovariate ShiftShiftShift ：此术语是google小组在论文BatchBatchBatch NormalizatoinNormalizatoinNormalizatoin 中提出来的，其主要描述的是：训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），此现象称之为Internal Covariate Shift. Batch Normalizatoin 之前的解决方案就是使用较小的学习率，和小心的初始化参数，对数据做白化处理，但是显然治标不治本

## $B^+$ 树
- $B^+$ 树与 B 树类似的结构, 但其概念和实现稍微简单一点, 在实际中使用得比 B 树更多.
- 定义: 一棵 m 阶的 $B^+$ 树或者空, 或者满足下面条件的树:
    1. 树中每个分支结点至多有 m 棵子树, 除根结点外的分支结点至多有 m 棵子树, 除根结点外的分支结点至少有 $\lfloor m/2 \rfloor$ 棵子树. 如果根结点不是叶结点, 至少有两棵子树.
    2. 关键码在结点里排序存放. 分支结点里的每一个关键结点里的每一个关键码关联着一棵子树, 这个关键码等于其所关联子树的根结点里的最大关键码. 叶结点里的每个关键码关联着一个数据项的存放位置, 数据项另行存储.

## 线性回归
- 线性模型
    - 线性模型 (linear model) 试图学得一个通过属性的线性组合来进行预测的函数
    - 线性模型形式简单, 易于建模, 许多非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得.
    - 由于 w 直观表达了各属性在预测中的重要性, 因此线性模型有很好的可解释性.
- 线性回归(linear regression) 试图学得一个线性模型以尽可能准确地预测实值输出标记
- 对离散属性, 若属性值间存在序关系, 可通过连续化将其转化为连续值
- 若属性减不存在序关系, 假定有 k 个属性值, 则通常转化为 k 维向量
- 广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行

## xgboost
- xgboost 是 boosting 算法中地一种.
    - boosting 算法地思想是将许多弱分类器集成在一起形成一个强分类器.
    - xgboost 是一种提升数模型, 它是将许多树模型集成在一起, 形成一个很强地分类器. 而所用到地树模型则是 CART 回归树模型.

- xgboost 算法思想
    - 该算法的思想就是不断地添加树, 不断地进行特征分裂来生长一棵树, 每次添加一棵树, 其实就是学习一个新的函数, 去拟合上次预测的残差. 当我们训练完成得到 k 棵树, 我们要预测一个样本的分数, 其实就是根据这个样本的特征, 在每棵树中会落在对应的一个叶子节点, 每个叶子节点就对应一个分数, 最后只要将每棵树对应的分数加起来就是该样本的预测值.
      $$
      \hat{y} = \phi{(x_i)} = \sum_{k=1}^{K}f_k(x_i)
      $$

      $$
      where \quad F = {f(x) = w_{q(x)}}(q: R^m \mapsto T, w \in R^T)
      $$
    
      
    
    - 注: $w_q(x)$为叶子节点 q 的分数, $f(x)$ 为其中一棵回归树
    
- xgboost 原理

- xgboost 目标函数定义为:
    $$
    Obj = \sum_{i=1}^{n}l(u_{i}, \hat{y_i})+\sum_{k=1}^{K}\Omega(f_k)
    $$
    目标函数由两部分构成, 第一部分用来衡量预测分数和真实分数的差距, 另一部分则是正则项. 正则项同样包含两部分, T 表示叶子节点的个数, w 表示叶子节点的分数. $\gamma$  可以控制叶子节点的个数, $\lambda$  可以控制叶子节点的分数不会过大, 防止过拟合.
    
- 正如上文所说, 新生成的树主要是要拟合上次预测的残差的, 即当生成 t 棵树后, 预测分数可以写成:
    $$
    \hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)
    $$
    同时, 可以将目标函数改写成:
    $$
    L^{(t)} = \sum_{i=1}^{n}l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)
    $$
    很明显, 我们接下来就是要去找到一个 $f_t$ 能够最小化目标函数. xgboost 的想法是利用其在 $f_t = 0$ 处的泰勒二阶展开近似它. 所以目标函数近似为:
    $$
    L^{(t)} \simeq \sum_{i=1}^n[l(y_i, \hat{y}_i^{(t-1)}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t)
    $$
    其中 $g_i$ 为一阶导数, $h_i$ 为二阶导数:
    $$
    g_i = \partial_{\hat{y}^{(t-1)}}l(y_i, \hat{y}^{(t-1)}), h_i = \partial_{\hat{y}^{(t-1)}}^2l(y_i, \hat{y}^{(t-1)})
    $$
    由于前 t-1 棵树的预测分数与 y 的残差对目标函数优化不影响, 可以直接去点, 简化目标函数为:
    $$
    \tilde{L}^{(t)} = \sum_{i=1}^n[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t)
    $$
    上式是将每个样本的损失函数值加起来, 我们知道, 每个样本都最终会落到一个叶子节点中, 所以我们可以将所有同一个叶子节点的样本重组起来, 过程如下图:

    

## CART 回归树
- CART 回归树假设树为二叉树, 通过将特征进行分裂. 比如当前树节点是基于第 j 个特征值进行分裂地, 设该特征值小于 s 的样本划分为左子树, 大于 s 的样本划分为右子树.
$$R_{1}(j, s) = {x|x^{(j)} \leq s}\quad and\quad R_{2}(j, s) = {x|x^{(j)} > s}$$
- CART 回归树实质上就是在该特征维度对样本空间进行划分, 而这种空间划分的优化是一种 NP 难问题, 因此, 在决策树模型中是使用启发式方法解决, 典型 CART 回归树产生的目标函数为:
$$\sum_{x_{i}\in R_{m}}(y_{i} - f(x_{i})^2)$$
- 因此, 当我们为了求解最优的划分特征 j 和最优的切分点 s, 就化为求解这么一个目标函数:
$$\min_{j,s}[\min_{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}(y_{i} - c_{1})^2 + \min_{c_2}\sum_{x_i \in R_2(j, s)} (y_i - c_2)^2]$$
- 所以我们只要遍历所有特征的所有切分点, 就能找到最优的切分特征和切分点, 最终得到一棵回归树.


